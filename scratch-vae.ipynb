{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94073a53-b355-43b3-8c8d-4d15ebef7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfc41fa-b884-4bf8-9361-3747344950e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport zizi_pipeline\n",
    "%aimport zizi_vae_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a1ad4f-8209-4a2b-ac4d-0c776bdb8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, VQModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from zizi_pipeline import get_dataloader, TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ab180a7-056f-4b7b-96a5-d24d48236371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11f16df30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(15926)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "489d31b1-c4e7-473a-a8b0-f142165ddd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig(\"data/pink-me/\", \"output/scratch-vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c09cb8a-b0f0-48e3-9b1c-e9b877adcd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = AutoencoderKL(\n",
    "    sample_size=config.image_size,\n",
    "    block_out_channels=(128,256,512,512),\n",
    "    down_block_types=(\n",
    "        \"DownEncoderBlock2D\",\n",
    "        \"DownEncoderBlock2D\",\n",
    "        \"DownEncoderBlock2D\",\n",
    "        \"DownEncoderBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpDecoderBlock2D\",\n",
    "        \"UpDecoderBlock2D\",\n",
    "        \"UpDecoderBlock2D\",\n",
    "        \"UpDecoderBlock2D\"\n",
    "    ),\n",
    "    latent_channels=4,\n",
    "    layers_per_block=2\n",
    ").to(\"mps\")\n",
    "vae.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a69039e-e97d-47db-92e0-9076a6d29a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e5ce2f-7582-4762-8bef-8f9c16e0dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4965835b-2e99-4ec4-aff4-c30ef9fbcc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_set = Subset(dataloader.dataset, range(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba4923d1-0b6f-410d-ab8b-26abeaa731e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loader = DataLoader(batch_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869672dd-56ea-48ca-b59f-a19dae83d4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0399,  0.0371,  0.0517,  ...,  0.0811,  0.0757,  0.0525],\n",
      "          [ 0.0208,  0.0102,  0.0298,  ...,  0.0829,  0.0656,  0.0437],\n",
      "          [ 0.0070,  0.0108,  0.0389,  ...,  0.0790,  0.0637,  0.0221],\n",
      "          ...,\n",
      "          [ 0.0262,  0.0301,  0.0805,  ...,  0.0514,  0.0254,  0.0079],\n",
      "          [ 0.0096,  0.0248,  0.0644,  ...,  0.0190, -0.0038, -0.0048],\n",
      "          [-0.0447, -0.0321, -0.0210,  ..., -0.0733, -0.0904, -0.0604]],\n",
      "\n",
      "         [[ 0.0313,  0.0277,  0.0303,  ...,  0.0290,  0.0308,  0.0248],\n",
      "          [ 0.0446,  0.0490,  0.0453,  ...,  0.0242,  0.0291,  0.0185],\n",
      "          [ 0.0487,  0.0460,  0.0277,  ...,  0.0131,  0.0308,  0.0213],\n",
      "          ...,\n",
      "          [ 0.0298,  0.0239,  0.0304,  ...,  0.0884,  0.0608,  0.0243],\n",
      "          [ 0.0304,  0.0285,  0.0339,  ...,  0.0941,  0.0746,  0.0371],\n",
      "          [ 0.0566,  0.0455,  0.0596,  ...,  0.0699,  0.0544,  0.0230]],\n",
      "\n",
      "         [[ 0.0088,  0.0079,  0.0091,  ...,  0.0317,  0.0115,  0.0033],\n",
      "          [ 0.0056,  0.0281,  0.0184,  ...,  0.0647,  0.0619,  0.0468],\n",
      "          [ 0.0206,  0.0813,  0.0678,  ...,  0.0882,  0.0683,  0.0480],\n",
      "          ...,\n",
      "          [ 0.0056,  0.0198, -0.0158,  ...,  0.0402,  0.0501,  0.0335],\n",
      "          [ 0.0017,  0.0135, -0.0117,  ...,  0.0382,  0.0446,  0.0521],\n",
      "          [ 0.0113,  0.0189,  0.0033,  ...,  0.0222,  0.0341,  0.0268]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0415,  0.0383,  0.0533,  ...,  0.0825,  0.0771,  0.0533],\n",
      "          [ 0.0219,  0.0108,  0.0309,  ...,  0.0846,  0.0674,  0.0452],\n",
      "          [ 0.0084,  0.0119,  0.0408,  ...,  0.0807,  0.0652,  0.0233],\n",
      "          ...,\n",
      "          [ 0.0274,  0.0318,  0.0836,  ...,  0.0543,  0.0269,  0.0093],\n",
      "          [ 0.0107,  0.0266,  0.0673,  ...,  0.0215, -0.0028, -0.0042],\n",
      "          [-0.0447, -0.0316, -0.0198,  ..., -0.0729, -0.0910, -0.0608]],\n",
      "\n",
      "         [[ 0.0310,  0.0280,  0.0305,  ...,  0.0297,  0.0314,  0.0260],\n",
      "          [ 0.0446,  0.0509,  0.0468,  ...,  0.0255,  0.0303,  0.0204],\n",
      "          [ 0.0486,  0.0475,  0.0283,  ...,  0.0141,  0.0318,  0.0228],\n",
      "          ...,\n",
      "          [ 0.0291,  0.0252,  0.0316,  ...,  0.0899,  0.0624,  0.0251],\n",
      "          [ 0.0300,  0.0298,  0.0350,  ...,  0.0963,  0.0765,  0.0385],\n",
      "          [ 0.0568,  0.0468,  0.0611,  ...,  0.0714,  0.0559,  0.0243]],\n",
      "\n",
      "         [[ 0.0079,  0.0063,  0.0082,  ...,  0.0307,  0.0107,  0.0025],\n",
      "          [ 0.0049,  0.0273,  0.0183,  ...,  0.0648,  0.0621,  0.0467],\n",
      "          [ 0.0199,  0.0811,  0.0679,  ...,  0.0879,  0.0681,  0.0474],\n",
      "          ...,\n",
      "          [ 0.0045,  0.0179, -0.0173,  ...,  0.0402,  0.0496,  0.0331],\n",
      "          [ 0.0005,  0.0114, -0.0139,  ...,  0.0374,  0.0437,  0.0517],\n",
      "          [ 0.0111,  0.0189,  0.0034,  ...,  0.0235,  0.0351,  0.0274]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0431,  0.0389,  0.0545,  ...,  0.0839,  0.0781,  0.0538],\n",
      "          [ 0.0234,  0.0110,  0.0316,  ...,  0.0865,  0.0684,  0.0460],\n",
      "          [ 0.0102,  0.0125,  0.0423,  ...,  0.0827,  0.0656,  0.0234],\n",
      "          ...,\n",
      "          [ 0.0297,  0.0338,  0.0860,  ...,  0.0566,  0.0272,  0.0100],\n",
      "          [ 0.0130,  0.0286,  0.0699,  ...,  0.0227, -0.0040, -0.0043],\n",
      "          [-0.0441, -0.0313, -0.0192,  ..., -0.0733, -0.0933, -0.0616]],\n",
      "\n",
      "         [[ 0.0317,  0.0294,  0.0316,  ...,  0.0310,  0.0331,  0.0269],\n",
      "          [ 0.0454,  0.0531,  0.0483,  ...,  0.0268,  0.0320,  0.0220],\n",
      "          [ 0.0495,  0.0493,  0.0293,  ...,  0.0156,  0.0342,  0.0247],\n",
      "          ...,\n",
      "          [ 0.0296,  0.0267,  0.0328,  ...,  0.0921,  0.0648,  0.0263],\n",
      "          [ 0.0308,  0.0318,  0.0366,  ...,  0.0991,  0.0800,  0.0403],\n",
      "          [ 0.0572,  0.0474,  0.0616,  ...,  0.0718,  0.0565,  0.0248]],\n",
      "\n",
      "         [[ 0.0079,  0.0046,  0.0069,  ...,  0.0308,  0.0107,  0.0011],\n",
      "          [ 0.0054,  0.0263,  0.0177,  ...,  0.0657,  0.0632,  0.0467],\n",
      "          [ 0.0199,  0.0805,  0.0679,  ...,  0.0891,  0.0693,  0.0474],\n",
      "          ...,\n",
      "          [ 0.0040,  0.0156, -0.0192,  ...,  0.0400,  0.0497,  0.0319],\n",
      "          [ 0.0002,  0.0094, -0.0160,  ...,  0.0365,  0.0431,  0.0509],\n",
      "          [ 0.0102,  0.0175,  0.0020,  ...,  0.0233,  0.0355,  0.0275]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0427,  0.0378,  0.0533,  ...,  0.0814,  0.0765,  0.0524],\n",
      "          [ 0.0231,  0.0106,  0.0310,  ...,  0.0836,  0.0660,  0.0452],\n",
      "          [ 0.0113,  0.0132,  0.0420,  ...,  0.0801,  0.0635,  0.0227],\n",
      "          ...,\n",
      "          [ 0.0292,  0.0329,  0.0849,  ...,  0.0563,  0.0261,  0.0101],\n",
      "          [ 0.0126,  0.0277,  0.0692,  ...,  0.0226, -0.0048, -0.0044],\n",
      "          [-0.0441, -0.0321, -0.0202,  ..., -0.0739, -0.0945, -0.0625]],\n",
      "\n",
      "         [[ 0.0317,  0.0295,  0.0316,  ...,  0.0311,  0.0330,  0.0268],\n",
      "          [ 0.0470,  0.0551,  0.0494,  ...,  0.0287,  0.0334,  0.0226],\n",
      "          [ 0.0510,  0.0517,  0.0305,  ...,  0.0178,  0.0360,  0.0254],\n",
      "          ...,\n",
      "          [ 0.0312,  0.0298,  0.0352,  ...,  0.0938,  0.0663,  0.0265],\n",
      "          [ 0.0330,  0.0356,  0.0395,  ...,  0.1009,  0.0817,  0.0404],\n",
      "          [ 0.0592,  0.0506,  0.0643,  ...,  0.0737,  0.0584,  0.0252]],\n",
      "\n",
      "         [[ 0.0066,  0.0026,  0.0051,  ...,  0.0291,  0.0090, -0.0006],\n",
      "          [ 0.0055,  0.0257,  0.0175,  ...,  0.0643,  0.0619,  0.0447],\n",
      "          [ 0.0201,  0.0795,  0.0676,  ...,  0.0874,  0.0674,  0.0447],\n",
      "          ...,\n",
      "          [ 0.0038,  0.0139, -0.0196,  ...,  0.0388,  0.0484,  0.0302],\n",
      "          [-0.0002,  0.0075, -0.0175,  ...,  0.0351,  0.0417,  0.0490],\n",
      "          [ 0.0112,  0.0173,  0.0020,  ...,  0.0229,  0.0346,  0.0265]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0436,  0.0394,  0.0547,  ...,  0.0830,  0.0776,  0.0540],\n",
      "          [ 0.0242,  0.0123,  0.0323,  ...,  0.0848,  0.0665,  0.0457],\n",
      "          [ 0.0122,  0.0150,  0.0430,  ...,  0.0814,  0.0645,  0.0237],\n",
      "          ...,\n",
      "          [ 0.0299,  0.0341,  0.0855,  ...,  0.0568,  0.0269,  0.0101],\n",
      "          [ 0.0126,  0.0282,  0.0691,  ...,  0.0234, -0.0032, -0.0034],\n",
      "          [-0.0438, -0.0315, -0.0203,  ..., -0.0732, -0.0930, -0.0622]],\n",
      "\n",
      "         [[ 0.0321,  0.0290,  0.0310,  ...,  0.0304,  0.0327,  0.0262],\n",
      "          [ 0.0479,  0.0546,  0.0492,  ...,  0.0278,  0.0328,  0.0215],\n",
      "          [ 0.0517,  0.0512,  0.0304,  ...,  0.0167,  0.0354,  0.0245],\n",
      "          ...,\n",
      "          [ 0.0323,  0.0298,  0.0346,  ...,  0.0923,  0.0651,  0.0260],\n",
      "          [ 0.0339,  0.0359,  0.0393,  ...,  0.0990,  0.0800,  0.0396],\n",
      "          [ 0.0589,  0.0496,  0.0628,  ...,  0.0718,  0.0567,  0.0240]],\n",
      "\n",
      "         [[ 0.0070,  0.0026,  0.0047,  ...,  0.0295,  0.0085, -0.0015],\n",
      "          [ 0.0054,  0.0253,  0.0163,  ...,  0.0639,  0.0608,  0.0432],\n",
      "          [ 0.0210,  0.0789,  0.0661,  ...,  0.0870,  0.0661,  0.0429],\n",
      "          ...,\n",
      "          [ 0.0048,  0.0135, -0.0207,  ...,  0.0387,  0.0480,  0.0284],\n",
      "          [ 0.0009,  0.0079, -0.0175,  ...,  0.0344,  0.0407,  0.0468],\n",
      "          [ 0.0121,  0.0177,  0.0020,  ...,  0.0222,  0.0337,  0.0253]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0436,  0.0396,  0.0547,  ...,  0.0827,  0.0775,  0.0534],\n",
      "          [ 0.0242,  0.0130,  0.0330,  ...,  0.0856,  0.0675,  0.0455],\n",
      "          [ 0.0121,  0.0155,  0.0436,  ...,  0.0814,  0.0648,  0.0233],\n",
      "          ...,\n",
      "          [ 0.0305,  0.0352,  0.0864,  ...,  0.0573,  0.0272,  0.0096],\n",
      "          [ 0.0131,  0.0291,  0.0698,  ...,  0.0238, -0.0030, -0.0039],\n",
      "          [-0.0432, -0.0303, -0.0195,  ..., -0.0726, -0.0925, -0.0622]],\n",
      "\n",
      "         [[ 0.0315,  0.0289,  0.0306,  ...,  0.0301,  0.0327,  0.0261],\n",
      "          [ 0.0467,  0.0546,  0.0485,  ...,  0.0278,  0.0330,  0.0213],\n",
      "          [ 0.0505,  0.0514,  0.0297,  ...,  0.0166,  0.0352,  0.0244],\n",
      "          ...,\n",
      "          [ 0.0313,  0.0296,  0.0340,  ...,  0.0932,  0.0661,  0.0258],\n",
      "          [ 0.0335,  0.0361,  0.0389,  ...,  0.0998,  0.0807,  0.0389],\n",
      "          [ 0.0582,  0.0492,  0.0622,  ...,  0.0717,  0.0563,  0.0233]],\n",
      "\n",
      "         [[ 0.0075,  0.0039,  0.0062,  ...,  0.0308,  0.0101, -0.0014],\n",
      "          [ 0.0061,  0.0266,  0.0181,  ...,  0.0653,  0.0623,  0.0429],\n",
      "          [ 0.0216,  0.0805,  0.0682,  ...,  0.0890,  0.0681,  0.0429],\n",
      "          ...,\n",
      "          [ 0.0056,  0.0147, -0.0189,  ...,  0.0400,  0.0491,  0.0280],\n",
      "          [ 0.0021,  0.0090, -0.0164,  ...,  0.0353,  0.0418,  0.0464],\n",
      "          [ 0.0129,  0.0179,  0.0025,  ...,  0.0230,  0.0343,  0.0249]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n",
      "prediction: DecoderOutput(sample=tensor([[[[ 0.0427,  0.0389,  0.0546,  ...,  0.0819,  0.0768,  0.0531],\n",
      "          [ 0.0228,  0.0121,  0.0325,  ...,  0.0843,  0.0668,  0.0461],\n",
      "          [ 0.0108,  0.0142,  0.0429,  ...,  0.0796,  0.0638,  0.0241],\n",
      "          ...,\n",
      "          [ 0.0293,  0.0349,  0.0861,  ...,  0.0577,  0.0274,  0.0110],\n",
      "          [ 0.0124,  0.0294,  0.0700,  ...,  0.0243, -0.0024, -0.0025],\n",
      "          [-0.0433, -0.0298, -0.0187,  ..., -0.0718, -0.0915, -0.0607]],\n",
      "\n",
      "         [[ 0.0311,  0.0283,  0.0298,  ...,  0.0300,  0.0324,  0.0252],\n",
      "          [ 0.0465,  0.0553,  0.0494,  ...,  0.0298,  0.0350,  0.0222],\n",
      "          [ 0.0504,  0.0522,  0.0308,  ...,  0.0183,  0.0371,  0.0252],\n",
      "          ...,\n",
      "          [ 0.0315,  0.0314,  0.0359,  ...,  0.0955,  0.0683,  0.0272],\n",
      "          [ 0.0336,  0.0374,  0.0400,  ...,  0.1015,  0.0824,  0.0401],\n",
      "          [ 0.0585,  0.0508,  0.0639,  ...,  0.0736,  0.0582,  0.0248]],\n",
      "\n",
      "         [[ 0.0073,  0.0038,  0.0064,  ...,  0.0312,  0.0110, -0.0010],\n",
      "          [ 0.0060,  0.0264,  0.0187,  ...,  0.0662,  0.0632,  0.0435],\n",
      "          [ 0.0211,  0.0799,  0.0685,  ...,  0.0893,  0.0685,  0.0431],\n",
      "          ...,\n",
      "          [ 0.0048,  0.0149, -0.0180,  ...,  0.0401,  0.0491,  0.0283],\n",
      "          [ 0.0016,  0.0095, -0.0159,  ...,  0.0357,  0.0419,  0.0465],\n",
      "          [ 0.0119,  0.0172,  0.0024,  ...,  0.0228,  0.0340,  0.0250]]]],\n",
      "       device='mps:0', grad_fn=<ConvolutionBackward0>))\n",
      "torch.Size([1, 3, 512, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m     predicted_img \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_img\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/autoencoder_kl.py:406\u001b[0m, in \u001b[0;36mAutoencoderKL.forward\u001b[0;34m(self, sample, sample_posterior, return_dict, generator)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     z \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39mmode()\n\u001b[0;32m--> 406\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/autoencoder_kl.py:264\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    262\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/autoencoder_kl.py:251\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiled_decode(z, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[1;32m    250\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 251\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/vae.py:265\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, z, latent_embeds)\u001b[0m\n\u001b[1;32m    262\u001b[0m             sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(create_custom_forward(up_block), sample, latent_embeds)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mto(upscale_dtype)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py:511\u001b[0m, in \u001b[0;36mUNetMidBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattentions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnets[\u001b[38;5;241m1\u001b[39m:]):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/attention_processor.py:321\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcross_attention_kwargs):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# The `Attention` class can call different attention processors / attention functions\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/mambaforge/base/envs/jupyter/lib/python3.9/site-packages/diffusers/models/attention_processor.py:1129\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb)\u001b[0m\n\u001b[1;32m   1125\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# TODO: add support for attn.scale when we move to Torch 2.1\u001b[39;00m\n\u001b[0;32m-> 1129\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m head_dim)\n\u001b[1;32m   1134\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step, img in enumerate(batch_loader):\n",
    "    print(img['images'].shape)\n",
    "    predicted_img = vae(img['images'].to(\"mps\"))\n",
    "    print(f\"prediction: {predicted_img}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb2fb1-1028-4855-a291-dea4a2e3b5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
